{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(timestamp):\n",
    "    \"\"\"\n",
    "    'YYYY-MM-DD HH:MM:SS AM/PM', 'YYYY-MM-DD HH:MM:SS', 또는 'YYYY-MM-DD HH:MM' 형식을 파싱하는 함수\n",
    "    \"\"\"\n",
    "    timestamp = timestamp.split('.')[0]\n",
    "    try:\n",
    "        # 먼저 24시간 형식으로 파싱 시도\n",
    "        return datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # 실패하면 12시간 형식으로 파싱 시도\n",
    "            return datetime.strptime(timestamp, '%Y-%m-%d %I:%M:%S %p')\n",
    "        except ValueError:\n",
    "            # 실패하면 'YYYY-MM-DD HH:MM' 형식으로 파싱 시도\n",
    "            return datetime.strptime(timestamp, '%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bus_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MASK_SELECTED                 int64\n",
       "LAT                         float64\n",
       "LNG                         float64\n",
       "STOP_ID                       int64\n",
       "Parsed_Date                  object\n",
       "up_down                       int64\n",
       "temperature                 float64\n",
       "Relative_Humidity           float64\n",
       "is_weekend                     bool\n",
       "day_name                     object\n",
       "previous                    float64\n",
       "wind_d                      float64\n",
       "wind_s                      float64\n",
       "Bus_num                       int64\n",
       "prev_arrive_time             object\n",
       "start_time                   object\n",
       "travel_time                 float64\n",
       "timestamp            datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_df['timestamp'] = bus_df['Parsed_Date'].apply(parse_time)\n",
    "bus_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_data(data, sequence_length=14, train_ratio=0.8, val_ratio=0.15):\n",
    "    # 주기성 변환\n",
    "    data[\"stop_sin\"] = np.sin(2 * np.pi * data[\"MASK_SELECTED\"] / sequence_length)\n",
    "    data[\"stop_cos\"] = np.cos(2 * np.pi * data[\"MASK_SELECTED\"] / sequence_length)\n",
    "    data[\"hour\"] = data[\"timestamp\"].dt.hour\n",
    "    data[\"hour_sin\"] = np.sin(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"hour_cos\"] = np.cos(2 * np.pi * data[\"hour\"] / 24)\n",
    "    \n",
    "    # 원핫 인코딩 적용\n",
    "    data = pd.get_dummies(data, columns=['day_name', ])\n",
    "   \n",
    "\n",
    "    # 필요없는 열 제거 및 텐서 변환 준비\n",
    "    features = data[[\n",
    "        \"MASK_SELECTED\", \"temperature\", \"LAT\", \"LNG\", \"Relative_Humidity\", 'wind_d', 'wind_s', 'is_weekend',\n",
    "        \"previous\", \"stop_sin\", \"hour_sin\", 'day_name_Monday', 'day_name_Saturday', 'day_name_Sunday',\n",
    "       'day_name_Thursday', 'day_name_Tuesday', 'day_name_Wednesday'\n",
    "    ]].values\n",
    "    target = data[\"travel_time\"].values.reshape(-1, 1)\n",
    "\n",
    "    # 스케일링 적용\n",
    "    scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = scaler_features.fit_transform(features)  # timestamp 포함\n",
    "    target_scaled = scaler_target.fit_transform(target)\n",
    "\n",
    "    # 데이터셋 분할 및 시퀀스 생성 함수\n",
    "    num_sequences = len(features_scaled) // sequence_length\n",
    "    train_size = int(num_sequences * train_ratio) * sequence_length\n",
    "    val_size = int(num_sequences * val_ratio) * sequence_length\n",
    "    \n",
    "    X_train = features_scaled[:train_size]\n",
    "    y_train = target_scaled[:train_size]\n",
    "    \n",
    "    X_val = features_scaled[train_size:train_size + val_size]\n",
    "    y_val = target_scaled[train_size:train_size + val_size]\n",
    "    \n",
    "    X_test = features_scaled[train_size + val_size:]\n",
    "    y_test = target_scaled[train_size + val_size:]\n",
    "    \n",
    "    # 시퀀스 데이터 생성 함수\n",
    "    def create_sequences(X, y, seq_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(0, len(X) - seq_length + 1, seq_length):\n",
    "            sequences.append(X[i:i + seq_length])\n",
    "            targets.append(y[i:i + seq_length])\n",
    "        return torch.tensor(np.array(sequences), dtype=torch.float32), torch.tensor(np.array(targets), dtype=torch.float32)\n",
    "\n",
    "    # 학습, 검증, 테스트 데이터셋 생성\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val, y_val, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# DataLoader 생성\n",
    "def create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    y_train = torch.tensor(y_train).float()\n",
    "    X_val = torch.tensor(X_val).float()\n",
    "    y_val = torch.tensor(y_val).float()\n",
    "    X_test = torch.tensor(X_test).float()\n",
    "    y_test = torch.tensor(y_test).float()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    # batch_size=None 또는 데이터셋 크기만큼 설정\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Lstm(nn.Module):\n",
    "    def __init__(self, num_stops=28, input_size=20, hidden_size=64):\n",
    "        super(Lstm, self).__init__()\n",
    "        \n",
    "        # LSTM 레이어\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        # Fully Connected 레이어\n",
    "        self.fc1 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 첫 번째 LSTM 레이어\n",
    "        lstm_1, _ = self.lstm1(x)  # lstm_1: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # 두 번째 Fully Connected 레이어\n",
    "        output = self.fc1(lstm_1)  # (batch_size, seq_len, 1)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=20, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): 얼마나 많은 에폭 동안 검증 손실이 개선되지 않아도 학습을 계속할지.\n",
    "            verbose (bool): True일 경우, 각 개선 사항을 출력.\n",
    "            delta (float): 개선으로 간주하기 위한 최소 변화.\n",
    "            path (str): 최상의 모델을 저장할 경로.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  # 손실이 낮을수록 좋으므로 음수로 변환\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''검증 손실이 감소하면 모델을 저장'''\n",
    "        if self.verbose:\n",
    "            print(\"===================================saved===================================\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.0382880049375152, Val Loss: 0.03207904410858949\n",
      "===================================saved===================================\n",
      "Epoch 2/1000, Train Loss: 0.025798548877771412, Val Loss: 0.040225209845673474\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 3/1000, Train Loss: 0.023163177147840282, Val Loss: 0.025680649404724438\n",
      "===================================saved===================================\n",
      "Epoch 4/1000, Train Loss: 0.018285269362552623, Val Loss: 0.01960666717163154\n",
      "===================================saved===================================\n",
      "Epoch 5/1000, Train Loss: 0.015421992621018685, Val Loss: 0.020092510973058995\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 6/1000, Train Loss: 0.01378362626668864, Val Loss: 0.018185841629192942\n",
      "===================================saved===================================\n",
      "Epoch 7/1000, Train Loss: 0.011581686125802142, Val Loss: 0.01475589256733656\n",
      "===================================saved===================================\n",
      "Epoch 8/1000, Train Loss: 0.010452591269443343, Val Loss: 0.014625689059141137\n",
      "===================================saved===================================\n",
      "Epoch 9/1000, Train Loss: 0.01015178395235645, Val Loss: 0.013840895345700639\n",
      "===================================saved===================================\n",
      "Epoch 10/1000, Train Loss: 0.009828859358094633, Val Loss: 0.012270415844839243\n",
      "===================================saved===================================\n",
      "Epoch 11/1000, Train Loss: 0.009734137530488494, Val Loss: 0.011821271541217962\n",
      "===================================saved===================================\n",
      "Epoch 12/1000, Train Loss: 0.009664916198484466, Val Loss: 0.011608677361870096\n",
      "===================================saved===================================\n",
      "Epoch 13/1000, Train Loss: 0.00975031365565623, Val Loss: 0.011368019306766135\n",
      "===================================saved===================================\n",
      "Epoch 14/1000, Train Loss: 0.009762284928001463, Val Loss: 0.011328306248677629\n",
      "===================================saved===================================\n",
      "Epoch 15/1000, Train Loss: 0.009643022985463696, Val Loss: 0.011266741502497877\n",
      "===================================saved===================================\n",
      "Epoch 16/1000, Train Loss: 0.009524386401088642, Val Loss: 0.011149035190187749\n",
      "===================================saved===================================\n",
      "Epoch 17/1000, Train Loss: 0.009468572311951513, Val Loss: 0.011133581843404542\n",
      "===================================saved===================================\n",
      "Epoch 18/1000, Train Loss: 0.00941900811864928, Val Loss: 0.011168227087528933\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 19/1000, Train Loss: 0.009379640433637957, Val Loss: 0.011187798077506679\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 20/1000, Train Loss: 0.009359441947058908, Val Loss: 0.01123105940808143\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 21/1000, Train Loss: 0.009333970203962443, Val Loss: 0.011273752387967847\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 22/1000, Train Loss: 0.009302534100632849, Val Loss: 0.011296418074163653\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 23/1000, Train Loss: 0.009272373370809614, Val Loss: 0.011329855015944867\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 24/1000, Train Loss: 0.009240970770146564, Val Loss: 0.01136431009286926\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 25/1000, Train Loss: 0.009208992751415021, Val Loss: 0.011398699799818652\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 26/1000, Train Loss: 0.009180059531770115, Val Loss: 0.01144392846063489\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 27/1000, Train Loss: 0.009153472049677345, Val Loss: 0.011487627712388834\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 28/1000, Train Loss: 0.009128468005136321, Val Loss: 0.011530335682133833\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 29/1000, Train Loss: 0.0091059025767858, Val Loss: 0.011574758128041313\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 30/1000, Train Loss: 0.009084711709874682, Val Loss: 0.011611756336476122\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 31/1000, Train Loss: 0.00906376849161461, Val Loss: 0.011642047958005042\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 32/1000, Train Loss: 0.009043243753174985, Val Loss: 0.011666700975703341\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 33/1000, Train Loss: 0.009022542718282369, Val Loss: 0.01168101890722201\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 34/1000, Train Loss: 0.009001107942562417, Val Loss: 0.011686343234032393\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 35/1000, Train Loss: 0.008979278824491692, Val Loss: 0.011684164937053407\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 36/1000, Train Loss: 0.008957050430970932, Val Loss: 0.011673383836057923\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 37/1000, Train Loss: 0.008934385629670163, Val Loss: 0.011656072101600114\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = bus_df\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, scaler_target = preprocess_data(data)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader, val_loader, test_loader = create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "model = Lstm(input_size= 17).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "# Early Stopping 초기화\n",
    "early_stopping = EarlyStopping(patience=20, verbose=True, path='batch32/lstm_model.pt')\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            output = model(\n",
    "            batch_X\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                \n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                output = model(\n",
    "                   batch_X\n",
    "                )\n",
    "                \n",
    "                loss = criterion(output, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "        early_stopping(val_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "# 학습 실행\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1213.7093505859375\n",
      "Mean Squared Error (RMSE): 34.83833312988281\n",
      "예측 결과가 test_predictions.csv 파일로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413452/970614050.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('batch32/lstm_model.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 최적의 모델 로드\n",
    "model.load_state_dict(torch.load('batch32/lstm_model.pt'))\n",
    "\n",
    "# 모델 평가 모드 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 결과와 실제값을 저장할 리스트 생성\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# 예측 및 실제값 계산\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 배치에서 수치형 데이터와 타겟 값 분리\n",
    "        batch_X, batch_y = batch[0].to(device), batch[1].to(device)\n",
    "        batch_X = batch_X\n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "\n",
    "        # 출력값과 타겟값의 차원을 일관되게 유지\n",
    "        output = output.squeeze(-1)  # 예측값에서 불필요한 마지막 차원 제거\n",
    "        batch_y = batch_y.squeeze(-1)  # 타겟값에서도 불필요한 마지막 차원 제거\n",
    "\n",
    "        # 예측값과 실제값을 리스트에 저장\n",
    "        predictions.append(output.cpu().numpy())  # GPU에서 CPU로 이동 후 numpy 변환\n",
    "        actuals.append(batch_y.cpu().numpy())     # GPU에서 CPU로 이동 후 numpy 변환\n",
    "\n",
    "# Flatten and inverse scale\n",
    "predictions = np.concatenate(predictions, axis=0)  # 배치별 예측값을 하나의 배열로 결합\n",
    "actuals = np.concatenate(actuals, axis=0)          # 배치별 실제값을 하나의 배열로 결합\n",
    "\n",
    "# 예측값과 실제값을 2차원 배열로 변환 (MinMaxScaler 사용을 위해)\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "# 예측값과 실제값의 스케일 복원\n",
    "predictions_original = scaler_target.inverse_transform(predictions)\n",
    "actuals_original = scaler_target.inverse_transform(actuals)\n",
    "\n",
    "# 데이터프레임으로 저장\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": actuals_original.flatten(),\n",
    "    \"Predicted\": predictions_original.flatten()\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "results_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(actuals_original, predictions_original)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Squared Error (RMSE): {rmse}\")\n",
    "print(\"예측 결과가 test_predictions.csv 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 2094.580322265625\n",
      "Mean Squared Error (RMSE): 45.76658630371094\n",
      "예측 결과가 test_predictions.csv 파일로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343742/970614050.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('batch32/lstm_model.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 최적의 모델 로드\n",
    "model.load_state_dict(torch.load('batch32/lstm_model.pt'))\n",
    "\n",
    "# 모델 평가 모드 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 결과와 실제값을 저장할 리스트 생성\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# 예측 및 실제값 계산\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 배치에서 수치형 데이터와 타겟 값 분리\n",
    "        batch_X, batch_y = batch[0].to(device), batch[1].to(device)\n",
    "        batch_X = batch_X\n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "\n",
    "        # 출력값과 타겟값의 차원을 일관되게 유지\n",
    "        output = output.squeeze(-1)  # 예측값에서 불필요한 마지막 차원 제거\n",
    "        batch_y = batch_y.squeeze(-1)  # 타겟값에서도 불필요한 마지막 차원 제거\n",
    "\n",
    "        # 예측값과 실제값을 리스트에 저장\n",
    "        predictions.append(output.cpu().numpy())  # GPU에서 CPU로 이동 후 numpy 변환\n",
    "        actuals.append(batch_y.cpu().numpy())     # GPU에서 CPU로 이동 후 numpy 변환\n",
    "\n",
    "# Flatten and inverse scale\n",
    "predictions = np.concatenate(predictions, axis=0)  # 배치별 예측값을 하나의 배열로 결합\n",
    "actuals = np.concatenate(actuals, axis=0)          # 배치별 실제값을 하나의 배열로 결합\n",
    "\n",
    "# 예측값과 실제값을 2차원 배열로 변환 (MinMaxScaler 사용을 위해)\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "# 예측값과 실제값의 스케일 복원\n",
    "predictions_original = scaler_target.inverse_transform(predictions)\n",
    "actuals_original = scaler_target.inverse_transform(actuals)\n",
    "\n",
    "# 데이터프레임으로 저장\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": actuals_original.flatten(),\n",
    "    \"Predicted\": predictions_original.flatten()\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "results_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(actuals_original, predictions_original)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Squared Error (RMSE): {rmse}\")\n",
    "print(\"예측 결과가 test_predictions.csv 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test2(Sequence X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 2171.5361328125\n",
      "Mean Squared Error (RMSE): 46.5997428894043\n",
      "예측 결과가 test_predictions.csv 파일로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_343742/970614050.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('batch32/lstm_model.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 최적의 모델 로드\n",
    "model.load_state_dict(torch.load('batch32/lstm_model.pt'))\n",
    "\n",
    "# 모델 평가 모드 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 결과와 실제값을 저장할 리스트 생성\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# 예측 및 실제값 계산\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 배치에서 수치형 데이터와 타겟 값 분리\n",
    "        batch_X, batch_y = batch[0].to(device), batch[1].to(device)\n",
    "        batch_X = batch_X\n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "\n",
    "        # 출력값과 타겟값의 차원을 일관되게 유지\n",
    "        output = output.squeeze(-1)  # 예측값에서 불필요한 마지막 차원 제거\n",
    "        batch_y = batch_y.squeeze(-1)  # 타겟값에서도 불필요한 마지막 차원 제거\n",
    "\n",
    "        # 예측값과 실제값을 리스트에 저장\n",
    "        predictions.append(output.cpu().numpy())  # GPU에서 CPU로 이동 후 numpy 변환\n",
    "        actuals.append(batch_y.cpu().numpy())     # GPU에서 CPU로 이동 후 numpy 변환\n",
    "\n",
    "# Flatten and inverse scale\n",
    "predictions = np.concatenate(predictions, axis=0)  # 배치별 예측값을 하나의 배열로 결합\n",
    "actuals = np.concatenate(actuals, axis=0)          # 배치별 실제값을 하나의 배열로 결합\n",
    "\n",
    "# 예측값과 실제값을 2차원 배열로 변환 (MinMaxScaler 사용을 위해)\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "# 예측값과 실제값의 스케일 복원\n",
    "predictions_original = scaler_target.inverse_transform(predictions)\n",
    "actuals_original = scaler_target.inverse_transform(actuals)\n",
    "\n",
    "# 데이터프레임으로 저장\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": actuals_original.flatten(),\n",
    "    \"Predicted\": predictions_original.flatten()\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "results_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(actuals_original, predictions_original)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Squared Error (RMSE): {rmse}\")\n",
    "print(\"예측 결과가 test_predictions.csv 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test1 Don't touch it (Sequence X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1983.3328857421875\n",
      "Mean Squared Error (RMSE): 44.53462600708008\n",
      "예측 결과가 test_predictions.csv 파일로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_300572/970614050.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('batch32/lstm_model.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 최적의 모델 로드\n",
    "model.load_state_dict(torch.load('batch32/lstm_model.pt'))\n",
    "\n",
    "# 모델 평가 모드 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 결과와 실제값을 저장할 리스트 생성\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# 예측 및 실제값 계산\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 배치에서 수치형 데이터와 타겟 값 분리\n",
    "        batch_X, batch_y = batch[0].to(device), batch[1].to(device)\n",
    "        batch_X = batch_X\n",
    "        # 모델 예측\n",
    "        output = model(batch_X)\n",
    "\n",
    "        # 출력값과 타겟값의 차원을 일관되게 유지\n",
    "        output = output.squeeze(-1)  # 예측값에서 불필요한 마지막 차원 제거\n",
    "        batch_y = batch_y.squeeze(-1)  # 타겟값에서도 불필요한 마지막 차원 제거\n",
    "\n",
    "        # 예측값과 실제값을 리스트에 저장\n",
    "        predictions.append(output.cpu().numpy())  # GPU에서 CPU로 이동 후 numpy 변환\n",
    "        actuals.append(batch_y.cpu().numpy())     # GPU에서 CPU로 이동 후 numpy 변환\n",
    "\n",
    "# Flatten and inverse scale\n",
    "predictions = np.concatenate(predictions, axis=0)  # 배치별 예측값을 하나의 배열로 결합\n",
    "actuals = np.concatenate(actuals, axis=0)          # 배치별 실제값을 하나의 배열로 결합\n",
    "\n",
    "# 예측값과 실제값을 2차원 배열로 변환 (MinMaxScaler 사용을 위해)\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "# 예측값과 실제값의 스케일 복원\n",
    "predictions_original = scaler_target.inverse_transform(predictions)\n",
    "actuals_original = scaler_target.inverse_transform(actuals)\n",
    "\n",
    "# 데이터프레임으로 저장\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": actuals_original.flatten(),\n",
    "    \"Predicted\": predictions_original.flatten()\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "results_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(actuals_original, predictions_original)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Squared Error (RMSE): {rmse}\")\n",
    "print(\"예측 결과가 test_predictions.csv 파일로 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
