{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(timestamp):\n",
    "    \"\"\"\n",
    "    'YYYY-MM-DD HH:MM:SS AM/PM', 'YYYY-MM-DD HH:MM:SS', 또는 'YYYY-MM-DD HH:MM' 형식을 파싱하는 함수\n",
    "    \"\"\"\n",
    "    timestamp = timestamp.split('.')[0]\n",
    "    try:\n",
    "        # 먼저 24시간 형식으로 파싱 시도\n",
    "        return datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return datetime.strptime(timestamp, '%Y-%m-%d %I:%M:%S %p')\n",
    "        except ValueError:\n",
    "            # 실패하면 'YYYY-MM-DD HH:MM' 형식으로 파싱 시도\n",
    "            return datetime.strptime(timestamp, '%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bus_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK_SELECTED        0\n",
      "LAT                  0\n",
      "LNG                  0\n",
      "STOP_ID              0\n",
      "Parsed_Date          0\n",
      "up_down              0\n",
      "temperature          0\n",
      "Relative_Humidity    0\n",
      "is_weekend           0\n",
      "day_name             0\n",
      "previous             0\n",
      "wind_d               0\n",
      "wind_s               0\n",
      "Bus_num              0\n",
      "prev_arrive_time     0\n",
      "start_time           0\n",
      "travel_time          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(bus_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MASK_SELECTED                 int64\n",
       "LAT                         float64\n",
       "LNG                         float64\n",
       "STOP_ID                       int64\n",
       "Parsed_Date                  object\n",
       "up_down                       int64\n",
       "temperature                 float64\n",
       "Relative_Humidity           float64\n",
       "is_weekend                     bool\n",
       "day_name                     object\n",
       "previous                    float64\n",
       "wind_d                      float64\n",
       "wind_s                      float64\n",
       "Bus_num                       int64\n",
       "prev_arrive_time             object\n",
       "start_time                   object\n",
       "travel_time                 float64\n",
       "timestamp            datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_df['timestamp'] = bus_df['Parsed_Date'].apply(parse_time)\n",
    "bus_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_data(data, sequence_length=14, train_ratio=0.8, val_ratio=0.15):\n",
    "    # 주기성 변환\n",
    "    data[\"stop_sin\"] = np.sin(2 * np.pi * data[\"MASK_SELECTED\"] / sequence_length)\n",
    "    data[\"stop_cos\"] = np.cos(2 * np.pi * data[\"MASK_SELECTED\"] / sequence_length)\n",
    "    data[\"hour\"] = data[\"timestamp\"].dt.hour\n",
    "    data[\"hour_sin\"] = np.sin(2 * np.pi * data[\"hour\"] / 24)\n",
    "    data[\"hour_cos\"] = np.cos(2 * np.pi * data[\"hour\"] / 24)\n",
    "    \n",
    "    # 원핫 인코딩 적용\n",
    "    data = pd.get_dummies(data, columns=['day_name', ])\n",
    "   \n",
    "\n",
    "    # 필요없는 열 제거 및 텐서 변환 준비\n",
    "    features = data[[\n",
    "        \"MASK_SELECTED\", \"temperature\", \"LAT\", \"LNG\", \"Relative_Humidity\", 'wind_d', 'wind_s', 'is_weekend',\n",
    "        \"previous\", \"stop_sin\", \"hour_sin\", 'day_name_Monday', 'day_name_Saturday', 'day_name_Sunday',\n",
    "       'day_name_Thursday', 'day_name_Tuesday', 'day_name_Wednesday'\n",
    "    ]].values\n",
    "    target = data[\"travel_time\"].values.reshape(-1, 1)\n",
    "\n",
    "    # 스케일링 적용\n",
    "    scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = scaler_features.fit_transform(features)  # timestamp 포함\n",
    "    target_scaled = scaler_target.fit_transform(target)\n",
    "    \"\"\"\n",
    "    # 데이터셋 분할 및 시퀀스 생성 함수\n",
    "    num_sequences = len(features_scaled) // sequence_length\n",
    "    train_size = int(num_sequences * train_ratio) * sequence_length\n",
    "    val_size = int(num_sequences * val_ratio) * sequence_length\n",
    "    \n",
    "    X_train = features_scaled[:train_size]\n",
    "    y_train = target_scaled[:train_size]\n",
    "    \n",
    "    X_val = features_scaled[train_size:train_size + val_size]\n",
    "    y_val = target_scaled[train_size:train_size + val_size]\n",
    "    \n",
    "    X_test = features_scaled[train_size + val_size:]\n",
    "    y_test = target_scaled[train_size + val_size:]\n",
    "    \"\"\"\n",
    "    \n",
    "    train_size = int(len(features_scaled) * 0.7)\n",
    "    val_size = int(len(features_scaled) * 0.2)\n",
    "    test_size = len(features_scaled) - train_size - val_size\n",
    "\n",
    "    X_train, X_val, X_test = features_scaled[0:train_size,:], features_scaled[train_size:train_size+val_size,:], features_scaled[train_size+val_size:len(features_scaled),:]\n",
    "    y_train, y_val, y_test = target_scaled[0:train_size,:], target_scaled[train_size:train_size+val_size,:], target_scaled[train_size+val_size:len(target_scaled),:]\n",
    "        \n",
    "    # 시퀀스 데이터 생성 함수\n",
    "    def create_sequences(X, y, seq_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(0, len(X) - seq_length + 1, seq_length):\n",
    "            sequences.append(X[i:i + seq_length])\n",
    "            targets.append(y[i:i + seq_length])\n",
    "        return torch.tensor(np.array(sequences), dtype=torch.float32), torch.tensor(np.array(targets), dtype=torch.float32)\n",
    "\n",
    "    # 학습, 검증, 테스트 데이터셋 생성\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val, y_val, sequence_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)\n",
    "    \n",
    "    return X_train_seq, y_train_seq, X_val_seq, y_val_seq, X_test_seq, y_test_seq, scaler_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# DataLoader 생성\n",
    "def create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    y_train = torch.tensor(y_train).float()\n",
    "    X_val = torch.tensor(X_val).float()\n",
    "    y_val = torch.tensor(y_val).float()\n",
    "    X_test = torch.tensor(X_test).float()\n",
    "    y_test = torch.tensor(y_test).float()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    # batch_size=None 또는 데이터셋 크기만큼 설정\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Stacked_Lstm(nn.Module):\n",
    "    def __init__(self, num_stops=28, input_size=15, hidden_size=16, dropout_prob=0.3):\n",
    "        super(Stacked_Lstm, self).__init__()\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=512, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=512, hidden_size=256, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=128, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(512)\n",
    "        self.layer_norm2 = nn.LayerNorm(256)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        # Fully Connected layers\n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_1, _ = self.lstm1(x)\n",
    "        lstm_1 = self.layer_norm1(lstm_1)\n",
    "        lstm_1 = self.dropout(lstm_1)\n",
    "        \n",
    "        lstm_2, _ = self.lstm2(lstm_1)\n",
    "        lstm_2 = self.layer_norm2(lstm_2)\n",
    "        lstm_2 = self.dropout(lstm_2)\n",
    "        \n",
    "        lstm_3, _ = self.lstm3(lstm_2)\n",
    "        lstm_3 = self.dropout(lstm_3)\n",
    "        \n",
    "        lstm_4, _ = self.lstm4(lstm_3)\n",
    "        lstm_4 = self.dropout(lstm_4)\n",
    "        \n",
    "        \n",
    "        output = self.fc1(lstm_4)  # (batch_size, seq_len, 1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=50, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): 얼마나 많은 에폭 동안 검증 손실이 개선되지 않아도 학습을 계속할지.\n",
    "            verbose (bool): True일 경우, 각 개선 사항을 출력.\n",
    "            delta (float): 개선으로 간주하기 위한 최소 변화.\n",
    "            path (str): 최상의 모델을 저장할 경로.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  # 손실이 낮을수록 좋으므로 음수로 변환\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''검증 손실이 감소하면 모델을 저장'''\n",
    "        if self.verbose:\n",
    "            print(\"===================================saved===================================\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413797/4158508312.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train).float()\n",
      "/tmp/ipykernel_413797/4158508312.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train).float()\n",
      "/tmp/ipykernel_413797/4158508312.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val).float()\n",
      "/tmp/ipykernel_413797/4158508312.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val).float()\n",
      "/tmp/ipykernel_413797/4158508312.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test).float()\n",
      "/tmp/ipykernel_413797/4158508312.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 0.05031997291371226, Val Loss: 0.034857381135225296\n",
      "===================================saved===================================\n",
      "Epoch 2/200, Train Loss: 0.036775270476937294, Val Loss: 0.04209090210497379\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 3/200, Train Loss: 0.03952181036584079, Val Loss: 0.03382503613829613\n",
      "===================================saved===================================\n",
      "Epoch 4/200, Train Loss: 0.03774074371904135, Val Loss: 0.04104020446538925\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 5/200, Train Loss: 0.036903990898281336, Val Loss: 0.03341168072074652\n",
      "===================================saved===================================\n",
      "Epoch 6/200, Train Loss: 0.03175481245853007, Val Loss: 0.03214003425091505\n",
      "===================================saved===================================\n",
      "Epoch 7/200, Train Loss: 0.031214430928230286, Val Loss: 0.029605451971292496\n",
      "===================================saved===================================\n",
      "Epoch 8/200, Train Loss: 0.02972934301942587, Val Loss: 0.028739637695252895\n",
      "===================================saved===================================\n",
      "Epoch 9/200, Train Loss: 0.026997826993465424, Val Loss: 0.029039259999990463\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 10/200, Train Loss: 0.02639115764759481, Val Loss: 0.026203814893960953\n",
      "===================================saved===================================\n",
      "Epoch 11/200, Train Loss: 0.025485337944701314, Val Loss: 0.024847506545484066\n",
      "===================================saved===================================\n",
      "Epoch 12/200, Train Loss: 0.024885770864784718, Val Loss: 0.02729106228798628\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 13/200, Train Loss: 0.026159496512264013, Val Loss: 0.021500245667994022\n",
      "===================================saved===================================\n",
      "Epoch 14/200, Train Loss: 0.02217805734835565, Val Loss: 0.02120354864746332\n",
      "===================================saved===================================\n",
      "Epoch 15/200, Train Loss: 0.02110881474800408, Val Loss: 0.01728690881282091\n",
      "===================================saved===================================\n",
      "Epoch 16/200, Train Loss: 0.017728013917803764, Val Loss: 0.02382895816117525\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 17/200, Train Loss: 0.02032687235623598, Val Loss: 0.015573244541883469\n",
      "===================================saved===================================\n",
      "Epoch 18/200, Train Loss: 0.023029926232993603, Val Loss: 0.016408240422606468\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 19/200, Train Loss: 0.0189363278914243, Val Loss: 0.016563317738473415\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 20/200, Train Loss: 0.017223284812644124, Val Loss: 0.012700836174190044\n",
      "===================================saved===================================\n",
      "Epoch 21/200, Train Loss: 0.014102337649092078, Val Loss: 0.011725443415343761\n",
      "===================================saved===================================\n",
      "Epoch 22/200, Train Loss: 0.013054064358584583, Val Loss: 0.011992760468274355\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 23/200, Train Loss: 0.012938369414769113, Val Loss: 0.010986053384840488\n",
      "===================================saved===================================\n",
      "Epoch 24/200, Train Loss: 0.011990095576038584, Val Loss: 0.01157552795484662\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 25/200, Train Loss: 0.012069707794580609, Val Loss: 0.011605872306972742\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 26/200, Train Loss: 0.012159019708633423, Val Loss: 0.01074717054143548\n",
      "===================================saved===================================\n",
      "Epoch 27/200, Train Loss: 0.010927187977358699, Val Loss: 0.010664638131856918\n",
      "===================================saved===================================\n",
      "Epoch 28/200, Train Loss: 0.01084639859618619, Val Loss: 0.010875246487557888\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 29/200, Train Loss: 0.010934131219983101, Val Loss: 0.010688094422221184\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 30/200, Train Loss: 0.010809924686327577, Val Loss: 0.010604011360555887\n",
      "===================================saved===================================\n",
      "Epoch 31/200, Train Loss: 0.011027118831407279, Val Loss: 0.010915886610746384\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 32/200, Train Loss: 0.01047010529146064, Val Loss: 0.010531788226217031\n",
      "===================================saved===================================\n",
      "Epoch 33/200, Train Loss: 0.010438731696922332, Val Loss: 0.010444649495184422\n",
      "===================================saved===================================\n",
      "Epoch 34/200, Train Loss: 0.01028767685056664, Val Loss: 0.010491610504686832\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 35/200, Train Loss: 0.010271901686792262, Val Loss: 0.01045622257515788\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 36/200, Train Loss: 0.010050721291918308, Val Loss: 0.01060337945818901\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 37/200, Train Loss: 0.010755382652860135, Val Loss: 0.011012645438313484\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 38/200, Train Loss: 0.011352634173817933, Val Loss: 0.010258060414344072\n",
      "===================================saved===================================\n",
      "Epoch 39/200, Train Loss: 0.010283650655765086, Val Loss: 0.010528972838073969\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 40/200, Train Loss: 0.010011111546191387, Val Loss: 0.010467542801052332\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 41/200, Train Loss: 0.009846835513599217, Val Loss: 0.010170398280024529\n",
      "===================================saved===================================\n",
      "Epoch 42/200, Train Loss: 0.00970823792158626, Val Loss: 0.010127692017704248\n",
      "===================================saved===================================\n",
      "Epoch 43/200, Train Loss: 0.009766145143657923, Val Loss: 0.009763266891241074\n",
      "===================================saved===================================\n",
      "Epoch 44/200, Train Loss: 0.009671984356828034, Val Loss: 0.010143379680812359\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 45/200, Train Loss: 0.009759329550433904, Val Loss: 0.009843579959124327\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 46/200, Train Loss: 0.009611253190087155, Val Loss: 0.009656443726271391\n",
      "===================================saved===================================\n",
      "Epoch 47/200, Train Loss: 0.008967774818302132, Val Loss: 0.009686051867902279\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 48/200, Train Loss: 0.00899962312541902, Val Loss: 0.0096362866461277\n",
      "===================================saved===================================\n",
      "Epoch 49/200, Train Loss: 0.00896677684795577, Val Loss: 0.009790913667529821\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 50/200, Train Loss: 0.009486666327575222, Val Loss: 0.009334730915725231\n",
      "===================================saved===================================\n",
      "Epoch 51/200, Train Loss: 0.009399573813425377, Val Loss: 0.009304345585405827\n",
      "===================================saved===================================\n",
      "Epoch 52/200, Train Loss: 0.009653616813011467, Val Loss: 0.00944844027981162\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 53/200, Train Loss: 0.00911700475262478, Val Loss: 0.009110897313803434\n",
      "===================================saved===================================\n",
      "Epoch 54/200, Train Loss: 0.00900781893869862, Val Loss: 0.009560040663927794\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 55/200, Train Loss: 0.008861336529662367, Val Loss: 0.009331835433840752\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 56/200, Train Loss: 0.008644205459859222, Val Loss: 0.009436625055968761\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 57/200, Train Loss: 0.008959495200542733, Val Loss: 0.009400335140526295\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 58/200, Train Loss: 0.009080710253329016, Val Loss: 0.009200144559144974\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 59/200, Train Loss: 0.008859095396474004, Val Loss: 0.009569588117301464\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 60/200, Train Loss: 0.009119465947151184, Val Loss: 0.009345981292426586\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 61/200, Train Loss: 0.00916513823904097, Val Loss: 0.014516294468194246\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 62/200, Train Loss: 0.011990889383014292, Val Loss: 0.01249903254210949\n",
      "EarlyStopping counter: 9 out of 20\n",
      "Epoch 63/200, Train Loss: 0.011040955316275358, Val Loss: 0.010971184819936752\n",
      "EarlyStopping counter: 10 out of 20\n",
      "Epoch 64/200, Train Loss: 0.00999265126301907, Val Loss: 0.01045455364510417\n",
      "EarlyStopping counter: 11 out of 20\n",
      "Epoch 65/200, Train Loss: 0.009542013533064164, Val Loss: 0.010194364003837109\n",
      "EarlyStopping counter: 12 out of 20\n",
      "Epoch 66/200, Train Loss: 0.009392272622790188, Val Loss: 0.00998067855834961\n",
      "EarlyStopping counter: 13 out of 20\n",
      "Epoch 67/200, Train Loss: 0.009287467313697562, Val Loss: 0.009794197045266628\n",
      "EarlyStopping counter: 14 out of 20\n",
      "Epoch 68/200, Train Loss: 0.009160699904896319, Val Loss: 0.009705231990665197\n",
      "EarlyStopping counter: 15 out of 20\n",
      "Epoch 69/200, Train Loss: 0.008824203250696883, Val Loss: 0.009564835578203201\n",
      "EarlyStopping counter: 16 out of 20\n",
      "Epoch 70/200, Train Loss: 0.008613874975708313, Val Loss: 0.009772648569196463\n",
      "EarlyStopping counter: 17 out of 20\n",
      "Epoch 71/200, Train Loss: 0.008588404438341968, Val Loss: 0.009652363602072\n",
      "EarlyStopping counter: 18 out of 20\n",
      "Epoch 72/200, Train Loss: 0.0083339544798946, Val Loss: 0.009482835419476032\n",
      "EarlyStopping counter: 19 out of 20\n",
      "Epoch 73/200, Train Loss: 0.008764500977122225, Val Loss: 0.009480634704232216\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = bus_df\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, scaler_target = preprocess_data(data)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader, val_loader, test_loader = create_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "# Instantiate the model\n",
    "model = Stacked_Lstm(input_size=17).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "early_stopping = EarlyStopping(patience=20, verbose=True, path='batch32/lstm_model2.pt')\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            output = model(\n",
    "            batch_X\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                \n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                output = model(\n",
    "                   batch_X\n",
    "                )\n",
    "                \n",
    "                loss = criterion(output, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "        early_stopping(val_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "# 학습 실행\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1042.1383056640625\n",
      "Root Mean Squared Error (RMSE): 32.28216552734375\n",
      "예측 결과가 test_predictions.csv 파일로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413797/3933972543.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('batch32/lstm_model2.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model.load_state_dict(torch.load('batch32/lstm_model2.pt'))\n",
    "# 모델 평가 모드 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 결과와 실제값을 저장할 리스트 생성\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# 예측 및 실제값 계산\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 배치에서 수치형 데이터와 타겟 값 분리\n",
    "        batch_X, batch_y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        output = model(batch_X)  # output: (batch_size, seq_len=1, 1)\n",
    "\n",
    "        # 출력값과 타겟값의 차원 맞추기\n",
    "        output = output.squeeze(-1).squeeze(-1)  # (batch_size,)\n",
    "        batch_y = batch_y.squeeze(-1)           # (batch_size,)\n",
    "\n",
    "        # 예측값과 실제값을 리스트에 저장\n",
    "        predictions.append(output.cpu().numpy())  # GPU에서 CPU로 이동 후 numpy 변환\n",
    "        actuals.append(batch_y.cpu().numpy())     # GPU에서 CPU로 이동 후 numpy 변환\n",
    "\n",
    "# 리스트를 하나의 배열로 결합\n",
    "predictions = np.concatenate(predictions, axis=0)  # (전체 샘플 수,)\n",
    "actuals = np.concatenate(actuals, axis=0)          # (전체 샘플 수,)\n",
    "\n",
    "# 예측값과 실제값을 2차원 배열로 변환 (MinMaxScaler 사용을 위해)\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "# 예측값과 실제값의 스케일 복원\n",
    "predictions_original = scaler_target.inverse_transform(predictions)\n",
    "actuals_original = scaler_target.inverse_transform(actuals)\n",
    "\n",
    "# 데이터프레임으로 저장\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": actuals_original.flatten(),\n",
    "    \"Predicted\": predictions_original.flatten()\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "results_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(actuals_original, predictions_original)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(\"예측 결과가 test_predictions.csv 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test1 don't touch it (Sequence O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1531.712646484375\n",
      "Root Mean Squared Error (RMSE): 39.13710021972656\n",
      "예측 결과가 test_predictions.csv 파일로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_301035/3933972543.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('batch32/lstm_model2.pt'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model.load_state_dict(torch.load('batch32/lstm_model2.pt'))\n",
    "# 모델 평가 모드 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 결과와 실제값을 저장할 리스트 생성\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# 예측 및 실제값 계산\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # 배치에서 수치형 데이터와 타겟 값 분리\n",
    "        batch_X, batch_y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        output = model(batch_X)  # output: (batch_size, seq_len=1, 1)\n",
    "\n",
    "        # 출력값과 타겟값의 차원 맞추기\n",
    "        output = output.squeeze(-1).squeeze(-1)  # (batch_size,)\n",
    "        batch_y = batch_y.squeeze(-1)           # (batch_size,)\n",
    "\n",
    "        # 예측값과 실제값을 리스트에 저장\n",
    "        predictions.append(output.cpu().numpy())  # GPU에서 CPU로 이동 후 numpy 변환\n",
    "        actuals.append(batch_y.cpu().numpy())     # GPU에서 CPU로 이동 후 numpy 변환\n",
    "\n",
    "# 리스트를 하나의 배열로 결합\n",
    "predictions = np.concatenate(predictions, axis=0)  # (전체 샘플 수,)\n",
    "actuals = np.concatenate(actuals, axis=0)          # (전체 샘플 수,)\n",
    "\n",
    "# 예측값과 실제값을 2차원 배열로 변환 (MinMaxScaler 사용을 위해)\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "# 예측값과 실제값의 스케일 복원\n",
    "predictions_original = scaler_target.inverse_transform(predictions)\n",
    "actuals_original = scaler_target.inverse_transform(actuals)\n",
    "\n",
    "# 데이터프레임으로 저장\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": actuals_original.flatten(),\n",
    "    \"Predicted\": predictions_original.flatten()\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "results_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(actuals_original, predictions_original)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(\"예측 결과가 test_predictions.csv 파일로 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bus_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
